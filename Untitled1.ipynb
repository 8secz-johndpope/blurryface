{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from stylegan import get_style_gan\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "import facenet_pytorch as fp\n",
    "import cv2, dlib\n",
    "import math\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def build_resnet_model(latent_space=512):\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    resnet.fc = nn.Sequential(*[\n",
    "        nn.Linear(512, 1024, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, latent_space, bias=True),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(1024, latent_space, bias=True)\n",
    "    ])\n",
    "    for param in resnet.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    resnet = resnet.cuda()\n",
    "\n",
    "    return resnet\n",
    "\n",
    "def crop_image(image, det):\n",
    "    left, top, right, bottom = rect_to_tuple(det)\n",
    "    return image[top:bottom, left:right]\n",
    "\n",
    "def rect_to_tuple(rect):\n",
    "    left = rect.left()\n",
    "    right = rect.right()\n",
    "    top = rect.top()\n",
    "    bottom = rect.bottom()\n",
    "    return left, top, right, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build face swap model\n",
    "import face_swap_py as fspy\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize face swap\n",
    "landmarks_path = 'data/shape_predictor_68_face_landmarks.dat'\n",
    "model_3dmm_h5_path = 'data/BaselFaceModel_mod_wForehead_noEars.h5'\n",
    "model_3dmm_dat_path = 'data/BaselFace.dat'\n",
    "reg_model_path = 'data/3dmm_cnn_resnet_101.caffemodel'\n",
    "reg_deploy_path = 'data/3dmm_cnn_resnet_101_deploy.prototxt'\n",
    "reg_mean_path = 'data/3dmm_cnn_resnet_101_mean.binaryproto'\n",
    "seg_model_path = 'data/face_seg_fcn8s.caffemodel'          # or 'data/face_seg_fcn8s_300.caffemodel' for lower resolution\n",
    "seg_deploy_path = 'data/face_seg_fcn8s_deploy.prototxt'    # or 'data/face_seg_fcn8s_300_deploy.prototxt' for lower resolution\n",
    "generic = False\n",
    "with_expr = True\n",
    "with_gpu = False\n",
    "gpu_device_id = 0\n",
    "fs = fspy.FaceSwap(landmarks_path, model_3dmm_h5_path,\n",
    "            model_3dmm_dat_path, reg_model_path,\n",
    "            reg_deploy_path, reg_mean_path,\n",
    "            seg_model_path, seg_deploy_path,\n",
    "            generic, with_expr, with_gpu, gpu_device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale = 4\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "num_eval = 5\n",
    "batch_size = 32\n",
    "fc_lr = 0.00005\n",
    "\n",
    "resnet = build_resnet_model()\n",
    "anonymizer = get_style_gan()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Now freeze the full model and then train only the fc layer\n",
    "for param in resnet.parameters(): # Unfreeze the full model\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "resnet.eval()\n",
    "resnet.fc.train()\n",
    "anonymizer.eval()\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda x: x.requires_grad, resnet.parameters())) , lr=fc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t\t Loss 1.0115655660629272\n",
      "Iteration: 20 \t\t Loss 0.9995635747909546\n",
      "Iteration: 40 \t\t Loss 1.0077414512634277\n",
      "Iteration: 60 \t\t Loss 1.0188370943069458\n",
      "Iteration: 80 \t\t Loss 1.0001838207244873\n",
      "Iteration: 100 \t\t Loss 0.9952874779701233\n",
      "Iteration: 120 \t\t Loss 1.0121955871582031\n",
      "Iteration: 140 \t\t Loss 1.0079536437988281\n",
      "Iteration: 160 \t\t Loss 0.9994035363197327\n",
      "Iteration: 180 \t\t Loss 1.0184910297393799\n",
      "Iteration: 200 \t\t Loss 0.9860864877700806\n",
      "Iteration: 220 \t\t Loss 1.01272714138031\n",
      "Iteration: 240 \t\t Loss 0.9952960014343262\n",
      "Iteration: 260 \t\t Loss 1.0131299495697021\n",
      "Iteration: 280 \t\t Loss 1.0127089023590088\n",
      "Iteration: 300 \t\t Loss 0.9963364601135254\n",
      "Iteration: 320 \t\t Loss 0.9804158210754395\n",
      "Iteration: 340 \t\t Loss 0.9991058111190796\n",
      "Iteration: 360 \t\t Loss 1.003040075302124\n",
      "Iteration: 380 \t\t Loss 0.9932920336723328\n",
      "Iteration: 400 \t\t Loss 1.0018343925476074\n",
      "Iteration: 420 \t\t Loss 1.0003981590270996\n",
      "Iteration: 440 \t\t Loss 1.0013539791107178\n",
      "Iteration: 460 \t\t Loss 0.9833095073699951\n",
      "Iteration: 480 \t\t Loss 1.0156464576721191\n",
      "Iteration: 500 \t\t Loss 0.9978762865066528\n",
      "Iteration: 520 \t\t Loss 0.9909887313842773\n",
      "Iteration: 540 \t\t Loss 1.010289192199707\n",
      "Iteration: 560 \t\t Loss 1.0050870180130005\n",
      "Iteration: 580 \t\t Loss 0.9817700386047363\n",
      "Iteration: 600 \t\t Loss 0.9779466390609741\n",
      "Iteration: 620 \t\t Loss 0.9716043472290039\n",
      "Iteration: 640 \t\t Loss 0.9973833560943604\n",
      "Iteration: 660 \t\t Loss 1.004065752029419\n",
      "Iteration: 680 \t\t Loss 1.0162460803985596\n",
      "Iteration: 700 \t\t Loss 1.0015943050384521\n",
      "Iteration: 720 \t\t Loss 1.0105684995651245\n",
      "Iteration: 740 \t\t Loss 0.9840602874755859\n",
      "Iteration: 760 \t\t Loss 0.9877239465713501\n",
      "Iteration: 780 \t\t Loss 0.9958921074867249\n",
      "Iteration: 800 \t\t Loss 1.0258376598358154\n",
      "Iteration: 820 \t\t Loss 0.9914426803588867\n",
      "Iteration: 840 \t\t Loss 1.0151774883270264\n",
      "Iteration: 860 \t\t Loss 0.9887491464614868\n",
      "Iteration: 880 \t\t Loss 1.00001859664917\n",
      "Iteration: 900 \t\t Loss 0.9862025380134583\n",
      "Iteration: 920 \t\t Loss 1.006056785583496\n",
      "Iteration: 940 \t\t Loss 1.0006895065307617\n",
      "Iteration: 960 \t\t Loss 0.9945718050003052\n",
      "Iteration: 980 \t\t Loss 1.0063802003860474\n",
      "Iteration: 1000 \t\t Loss 0.9956536293029785\n",
      "Iteration: 1020 \t\t Loss 0.9907643795013428\n",
      "Iteration: 1040 \t\t Loss 0.9772751331329346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-04383001e259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(0, 100000, 1):\n",
    "    resnet.train()\n",
    "    latents = torch.randn(batch_size, 512).cuda()\n",
    "    generated_image = anonymizer(latents)\n",
    "    generated_image = (generated_image.clamp(-1, 1) + 1) / 2.0\n",
    "    \n",
    "    generated_image = interpolate(generated_image, size=(224, 224)).cpu()\n",
    "    generated_image = torch.stack([normalize(x).cpu() for x in generated_image]).detach().cuda()\n",
    "    \n",
    "    predicted_features = resnet(generated_image)\n",
    "    \n",
    "    loss = loss_fn(predicted_features, latents) # we wanna make the latent features representative\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if j % 20 == 0:\n",
    "        print(f\"Iteration: {j} \\t\\t Loss {loss.item()}\")\n",
    "        facenet.eval()\n",
    "        fc.eval()\n",
    "        with torch.no_grad():\n",
    "            latents = torch.randn(5, 512).cuda()\n",
    "            generated_image = anonymizer(latents)\n",
    "            generated_image = (generated_image.clamp(-1, 1) + 1) / 2.0\n",
    "            generated_image = interpolate(generated_image, size=(224, 224)).cpu()\n",
    "            \n",
    "            generated_image_normalized = torch.stack([normalize(x).cpu() for x in generated_image]).detach().cuda()\n",
    "            predicted_features = resnet(generated_image_normalized)\n",
    "\n",
    "            resnet_based_images = anonymizer(predicted_features)\n",
    "            resnet_based_images = (resnet_based_images.clamp(-1, 1) + 1) / 2.0\n",
    "            resnet_based_images = interpolate(resnet_based_images, size=(224, 224)).cpu()\n",
    "            \n",
    "            uniform_rand = anonymizer(predicted_features + (torch.rand(5,512)/10.0).cuda())\n",
    "            uniform_rand = (uniform_rand.clamp(-1, 1) + 1) / 2.0\n",
    "            uniform_rand = interpolate(uniform_rand, size=(224, 224)).cpu()\n",
    "            \n",
    "            noise = torch.randn(5,512).cuda()\n",
    "            \n",
    "            lightly_p = anonymizer(predicted_features + (noise / 100.0))\n",
    "            lightly_p = (lightly_p.clamp(-1, 1) + 1) / 2.0\n",
    "            lightly_p = interpolate(lightly_p, size=(224, 224)).cpu()\n",
    "                \n",
    "            fd1 = fspy.FaceData(img1)\n",
    "            fd2 = fspy.FaceData(img2)\n",
    "            result_img = fs.swap(fd1, fd2)\n",
    "            \n",
    "            highly_p = anonymizer(predicted_features + (noise / 15.0))\n",
    "            highly_p = (highly_p.clamp(-1, 1) + 1) / 2.0\n",
    "            highly_p = interpolate(highly_p, size=(224, 224)).cpu()\n",
    "            \n",
    "            #### Swap faces in image ######\n",
    "            swapped_img = []\n",
    "            for source_img, anon_img in zip(generated_image, lightly_p):\n",
    "                anon_img = cv2.cvtColor(anon_img.numpy(), cv2.COLOR_RGB2BGR)\n",
    "                source_img = cv2.cvtColor(source_img.numpy(), cv2.COLOR_RGB2BGR)\n",
    "                fd1 = fspy.FaceData(img1)\n",
    "                fd2 = fspy.FaceData(img2)\n",
    "                result_img = fs.swap(fd1, fd2)\n",
    "                swapped_img.append(result_img)\n",
    "            swapped_imgs = torch.stack([torch.tensor(x) for x in swapped_img])\n",
    "                \n",
    "\n",
    "            images = torchvision.utils.make_grid(torch.cat([generated_image, uniform_rand, resnet_based_images, lightly_p, highly_p, swapped_imgs]), nrow=5)\n",
    "            torchvision.utils.save_image(images, \"input_output/\" + str(\n",
    "                j)  + \".png\", nrow=5, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_facenet_model(latent_space=512):\n",
    "#     mtcnn = fp.MTCNN(device=torch.device(\"cuda\"))\n",
    "#     facenet = fp.InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
    "#     for p in facenet.parameters():\n",
    "#         p.requires_grad = False\n",
    "\n",
    "\n",
    "#     fc = nn.Sequential(*[\n",
    "#        nn.Linear(512, 512, bias=True),\n",
    "#        nn.ReLU(),\n",
    "#     ])\n",
    "\n",
    "#     fc.train()\n",
    "#     fc = fc.cuda()\n",
    "\n",
    "#     return mtcnn, facenet, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
